{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9254ca4-093c-47b0-9ce0-80cf43cce7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/drive/MyDrive/DLStudio/DLStudio-2.4.2/ExamplesAdversarialLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da105c2-902a-40bb-aff0-4457d40f8202",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip celeba_dataset_64x64.zip -d /content/drive/MyDrive/DLStudio/DLStudio-2.4.2/ExamplesAdversarialLearning/celeba_dataset_64x64/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b907445-a780-467f-ba52-7e4065e3c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import torchvision.transforms as tvt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as tvt\n",
    "from torchvision import utils\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import skimage.io as io\n",
    "from pycocotools.coco import COCO\n",
    "import copy\n",
    "from scipy.ndimage import zoom\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "numpy.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmarks=False\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "sys.path.append( \"/content/drive/MyDrive/DLStudio/DLStudio-2.4.2/DLStudio/\" )\n",
    "sys.path.append( \"/content/drive/MyDrive/DLStudio/DLStudio-2.4.2/AdversarialLearning\" )\n",
    "##  watch -d -n 0.5 nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c258e49-55dd-4562-a9bd-001f4dcdf8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and Dataloader for CelebA subset dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = [os.path.join(root_dir, filename) for filename in os.listdir(root_dir)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # if torch.is_tensor(idx):\n",
    "        #     idx = idx.tolist()\n",
    "\n",
    "        img_name = self.image_filenames[idx]\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "# Example usage:\n",
    "data_transform = tvt.Compose([\n",
    "    tvt.Resize((64, 64)),\n",
    "    tvt.CenterCrop((64,64)),         \n",
    "    tvt.ToTensor(),\n",
    "    tvt.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),         \n",
    "])\n",
    "\n",
    "dataset = CustomImageDataset(root_dir='/content/drive/MyDrive/DLStudio/DLStudio-2.4.2/ExamplesAdversarialLearning/celeba_dataset_64x64/celeba_dataset_64x64', transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6df6ca5-12c3-4cd9-8ed8-e0d9283c4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76175a1f-2929-4a1b-9ea7-93f699d0451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code has been picked up from the DL Studio Library\n",
    "def weights_init(m):        \n",
    "            classname = m.__class__.__name__     \n",
    "            if classname.find('Conv') != -1:         \n",
    "                nn.init.normal_(m.weight.data, 0.0, 0.02)      \n",
    "            elif classname.find('BatchNorm') != -1:         \n",
    "                nn.init.normal_(m.weight.data, 1.0, 0.02)       \n",
    "                nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd0af27-bff7-4f1d-bc3f-39e368ec591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code has been picked up from the DL Studio Library\n",
    "class DiscriminatorDG1(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(DiscriminatorDG1, self).__init__()\n",
    "                self.conv_in = nn.Conv2d(  3,    64,      kernel_size=4,      stride=2,    padding=1)\n",
    "                self.conv_in2 = nn.Conv2d( 64,   128,     kernel_size=4,      stride=2,    padding=1)\n",
    "                self.conv_in3 = nn.Conv2d( 128,  256,     kernel_size=4,      stride=2,    padding=1)\n",
    "                self.conv_in4 = nn.Conv2d( 256,  512,     kernel_size=4,      stride=2,    padding=1)\n",
    "                self.conv_in5 = nn.Conv2d( 512,  1,       kernel_size=4,      stride=1,    padding=0)\n",
    "                self.bn1  = nn.BatchNorm2d(128)\n",
    "                self.bn2  = nn.BatchNorm2d(256)\n",
    "                self.bn3  = nn.BatchNorm2d(512)\n",
    "                self.sig = nn.Sigmoid()\n",
    "            def forward(self, x):                 \n",
    "                x = torch.nn.functional.leaky_relu(self.conv_in(x), negative_slope=0.2, inplace=True)\n",
    "                x = self.bn1(self.conv_in2(x))\n",
    "                x = torch.nn.functional.leaky_relu(x, negative_slope=0.2, inplace=True)\n",
    "                x = self.bn2(self.conv_in3(x))\n",
    "                x = torch.nn.functional.leaky_relu(x, negative_slope=0.2, inplace=True)\n",
    "                x = self.bn3(self.conv_in4(x))\n",
    "                x = torch.nn.functional.leaky_relu(x, negative_slope=0.2, inplace=True)\n",
    "                x = self.conv_in5(x)\n",
    "                x = self.sig(x)\n",
    "                return x\n",
    "\n",
    "class GeneratorDG1(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(GeneratorDG1, self).__init__()\n",
    "                self.latent_to_image = nn.ConvTranspose2d( 100,   512,  kernel_size=4, stride=1, padding=0, bias=False)\n",
    "                self.upsampler2 = nn.ConvTranspose2d( 512, 256, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "                self.upsampler3 = nn.ConvTranspose2d (256, 128, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "                self.upsampler4 = nn.ConvTranspose2d (128, 64,  kernel_size=4, stride=2, padding=1, bias=False)\n",
    "                self.upsampler5 = nn.ConvTranspose2d(  64,  3,  kernel_size=4, stride=2, padding=1, bias=False)\n",
    "                self.bn1 = nn.BatchNorm2d(512)\n",
    "                self.bn2 = nn.BatchNorm2d(256)\n",
    "                self.bn3 = nn.BatchNorm2d(128)\n",
    "                self.bn4 = nn.BatchNorm2d(64)\n",
    "                self.tanh  = nn.Tanh()\n",
    "            def forward(self, x):                     \n",
    "                x = self.latent_to_image(x)\n",
    "                x = torch.nn.functional.relu(self.bn1(x))\n",
    "                x = self.upsampler2(x)\n",
    "                x = torch.nn.functional.relu(self.bn2(x))\n",
    "                x = self.upsampler3(x)\n",
    "                x = torch.nn.functional.relu(self.bn3(x))\n",
    "                x = self.upsampler4(x)\n",
    "                x = torch.nn.functional.relu(self.bn4(x))\n",
    "                x = self.upsampler5(x)\n",
    "                x = self.tanh(x)\n",
    "                return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843faea5-04ee-4214-b3b5-e7520e7e4cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code has been picked up from the DL Studio Library with some changes from my end\n",
    "def run_gan_code(discriminator, generator, results_dir, dataloader):\n",
    "            dir_name_for_results = results_dir\n",
    "            if os.path.exists(dir_name_for_results):\n",
    "                files = glob.glob(dir_name_for_results + \"/*\")\n",
    "                for file in files:\n",
    "                    if os.path.isfile(file):\n",
    "                        os.remove(file)\n",
    "                    else:\n",
    "                        files = glob.glob(file + \"/*\")\n",
    "                        list(map(lambda x: os.remove(x), files))\n",
    "            else:\n",
    "                os.mkdir(dir_name_for_results)\n",
    "            #  Set the number of channels for the 1x1 input noise vectors for the Generator:\n",
    "            nz = 100\n",
    "            netD = discriminator.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            netG = generator.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "            #  Initialize the parameters of the Discriminator and the Generator networks according to the\n",
    "            #  definition of the \"weights_init()\" method:\n",
    "            netD.apply(weights_init)\n",
    "            netG.apply(weights_init)\n",
    "            #  We will use a the same noise batch to periodically check on the progress made for the Generator:\n",
    "            fixed_noise = torch.randn(32, nz, 1, 1, device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")          \n",
    "            #  Establish convention for real and fake labels during training\n",
    "            real_label = 1   \n",
    "            fake_label = 0         \n",
    "            #  Adam optimizers for the Discriminator and the Generator:\n",
    "            optimizerD = optim.Adam(netD.parameters(), lr=1e-4, betas=(0.9, 0.999))    \n",
    "            optimizerG = optim.Adam(netG.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "            #  Establish the criterion for measuring the loss at the output of the Discriminator network:\n",
    "            criterion = nn.BCELoss()\n",
    "            #  We will use these lists to store the results accumulated during training:\n",
    "            img_list = []                               \n",
    "            G_losses = []                               \n",
    "            D_losses = []                               \n",
    "            iters = 0                                   \n",
    "            print(\"\\n\\nStarting Training Loop...\\n\\n\")      \n",
    "            start_time = time.perf_counter()            \n",
    "            for epoch in range(30):        \n",
    "                g_losses_per_print_cycle = []           \n",
    "                d_losses_per_print_cycle = []           \n",
    "                # For each batch in the dataloader\n",
    "                for i, data in enumerate(dataloader, 0):         \n",
    "\n",
    "                    ##  Maximization Part of the Min-Max Objective of Eq. (3):\n",
    "                    ##\n",
    "                    ##  As indicated by Eq. (3) in the DCGAN part of the doc section at the beginning of this \n",
    "                    ##  file, the GAN training boils down to carrying out a min-max optimization. Each iterative \n",
    "                    ##  step of the max part results in updating the Discriminator parameters and each iterative \n",
    "                    ##  step of the min part results in the updating of the Generator parameters.  For each \n",
    "                    ##  batch of the training data, we first do max and then do min.  Since the max operation \n",
    "                    ##  affects both terms of the criterion shown in the doc section, it has two parts: In the\n",
    "                    ##  first part we apply the Discriminator to the training images using 1.0 as the target; \n",
    "                    ##  and, in the second part, we supply to the Discriminator the output of the Generator \n",
    "                    ##  and use 0 as the target. In what follows, the Discriminator is being applied to \n",
    "                    ##  the training images:\n",
    "                    netD.zero_grad()    \n",
    "                    real_images_in_batch = data[0].to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")     \n",
    "                    #  Need to know how many images we pulled in since at the tailend of the dataset, the \n",
    "                    #  number of images may not equal the user-specified batch size:\n",
    "                    b_size = real_images_in_batch.size(0)  \n",
    "                    label = torch.full((b_size,), real_label, dtype=torch.float, device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n",
    "                    output = netD(real_images_in_batch).view(-1)  \n",
    "                    lossD_for_reals = criterion(output, label)                                                   \n",
    "                    lossD_for_reals.backward()                                                                   \n",
    "                    ##  That brings us the second part of what it takes to carry out the max operation on the\n",
    "                    ##  min-max criterion shown in Eq. (3) in the doc section at the beginning of this file.\n",
    "                    ##  part calls for applying the Discriminator to the images produced by the Generator from \n",
    "                    ##  noise:\n",
    "                    noise = torch.randn(b_size, nz, 1, 1, device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n",
    "                    fakes = netG(noise) \n",
    "                    label.fill_(fake_label) \n",
    "                    ##  The call to fakes.detach() in the next statement returns a copy of the 'fakes' tensor \n",
    "                    ##  that does not exist in the computational graph. That is, the call shown below first \n",
    "                    ##  makes a copy of the 'fakes' tensor and then removes it from the computational graph. \n",
    "                    ##  The original 'fakes' tensor continues to remain in the computational graph.  This ploy \n",
    "                    ##  ensures that a subsequent call to backward() in the 3rd statement below would only\n",
    "                    ##  update the netD weights.\n",
    "                    output = netD(fakes.detach()).view(-1)    \n",
    "                    lossD_for_fakes = criterion(output, label)    \n",
    "                    ##  At this point, we do not care if the following call also calculates the gradients\n",
    "                    ##  wrt the Discriminator weights since we are going to next iteration with 'netD.zero_grad()':\n",
    "                    lossD_for_fakes.backward()          \n",
    "                    lossD = lossD_for_reals + lossD_for_fakes    \n",
    "                    d_losses_per_print_cycle.append(lossD)  \n",
    "                    ##  Only the Discriminator weights are incremented:\n",
    "                    optimizerD.step()  \n",
    "\n",
    "                    ##  Minimization Part of the Min-Max Objective of Eq. (3):\n",
    "                    ##\n",
    "                    ##  That brings to the min part of the max-min optimization described in Eq. (3) the doc \n",
    "                    ##  section at the beginning of this file.  The min part requires that we minimize \n",
    "                    ##  \"1 - D(G(z))\" which, since D is constrained to lie in the interval (0,1), requires that \n",
    "                    ##  we maximize D(G(z)).  We accomplish that by applying the Discriminator to the output \n",
    "                    ##  of the Generator and use 1 as the target for each image:\n",
    "                    netG.zero_grad()   \n",
    "                    label.fill_(real_label)  \n",
    "                    output = netD(fakes).view(-1)   \n",
    "                    lossG = criterion(output, label)          \n",
    "                    g_losses_per_print_cycle.append(lossG) \n",
    "                    lossG.backward()    \n",
    "                    ##  Only the Generator parameters are incremented:\n",
    "                    optimizerG.step() \n",
    "                    if i % 100 == 99:                                                                           \n",
    "                        current_time = time.perf_counter()                                                      \n",
    "                        elapsed_time = current_time - start_time                                                \n",
    "                        mean_D_loss = torch.mean(torch.FloatTensor(d_losses_per_print_cycle))                   \n",
    "                        mean_G_loss = torch.mean(torch.FloatTensor(g_losses_per_print_cycle))                   \n",
    "                        print(\"[epoch=%d/%d   iter=%4d   elapsed_time=%5d secs]     mean_D_loss=%7.4f    mean_G_loss=%7.4f\" % \n",
    "                                      ((epoch+1),30,(i+1),elapsed_time,mean_D_loss,mean_G_loss))   \n",
    "                        d_losses_per_print_cycle = []                                                           \n",
    "                        g_losses_per_print_cycle = []                                                           \n",
    "                    G_losses.append(lossG.item())                                                                \n",
    "                    D_losses.append(lossD.item())                                                                \n",
    "                    if (iters % 500 == 0) or ((epoch == 30-1) and (i == len(dataloader)-1)):   \n",
    "                        with torch.no_grad():             \n",
    "                            fake = netG(fixed_noise).detach().cpu()  ## detach() removes the fake from comp. graph. \n",
    "                                                                     ## for creating its CPU compatible version\n",
    "                        img_list.append(torchvision.utils.make_grid(fake, padding=1, pad_value=1, normalize=True))\n",
    "                    iters += 1              \n",
    "            #  At the end of training, make plots from the data in G_losses and D_losses:\n",
    "            plt.figure(figsize=(10,5))    \n",
    "            plt.title(\"Generator and Discriminator Loss During Training\")    \n",
    "            plt.plot(G_losses,label=\"G\")    \n",
    "            plt.plot(D_losses,label=\"D\") \n",
    "            plt.xlabel(\"iterations\")   \n",
    "            plt.ylabel(\"Loss\")         \n",
    "            plt.legend()          \n",
    "            plt.savefig(dir_name_for_results + \"/gen_and_disc_loss_training.png\") \n",
    "            plt.show()    \n",
    "            #  Make an animated gif from the Generator output images stored in img_list:            \n",
    "            images = []           \n",
    "            for imgobj in img_list:  \n",
    "                img = tvtF.to_pil_image(imgobj)  \n",
    "                images.append(img) \n",
    "            imageio.mimsave(dir_name_for_results + \"/generation_animation.gif\", images, fps=5)\n",
    "            #  Make a side-by-side comparison of a batch-size sampling of real images drawn from the\n",
    "            #  training data and what the Generator is capable of producing at the end of training:\n",
    "            real_batch = next(iter(dataloader)) \n",
    "            real_batch = real_batch[0]\n",
    "            plt.figure(figsize=(15,15))  \n",
    "            plt.subplot(1,2,1)   \n",
    "            plt.axis(\"off\")   \n",
    "            plt.title(\"Real Images\")    \n",
    "            plt.imshow(np.transpose(torchvision.utils.make_grid(real_batch.to(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"), \n",
    "                                                   padding=1, pad_value=1, normalize=True).cpu(),(1,2,0)))  \n",
    "            plt.subplot(1,2,2)                                                                             \n",
    "            plt.axis(\"off\")                                                                                \n",
    "            plt.title(\"Fake Images\")                                                                       \n",
    "            plt.imshow(np.transpose(img_list[-1],(1,2,0)))                                                 \n",
    "            plt.savefig(dir_name_for_results + \"/real_vs_fake_images.png\")                                 \n",
    "            plt.show() \n",
    "            torch.save(netG.state_dict(), \"/content/drive/MyDrive/DLStudio-2.4.2/ExamplesAdversarialLearning/results_DG1/save_model_loss_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a7f33-a6b2-4e5f-9f4a-69327d9d292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator =  DiscriminatorDG1()\n",
    "generator =  GeneratorDG1()\n",
    "\n",
    "num_learnable_params_disc = sum(p.numel() for p in discriminator.parameters() if p.requires_grad)\n",
    "print(\"\\n\\nThe number of learnable parameters in the Discriminator: %d\\n\" % num_learnable_params_disc)\n",
    "num_learnable_params_gen = sum(p.numel() for p in generator.parameters() if p.requires_grad)\n",
    "print(\"\\nThe number of learnable parameters in the Generator: %d\\n\" % num_learnable_params_gen)\n",
    "num_layers_disc = len(list(discriminator.parameters()))\n",
    "print(\"\\nThe number of layers in the discriminator: %d\\n\" % num_layers_disc)\n",
    "num_layers_gen = len(list(generator.parameters()))\n",
    "print(\"\\nThe number of layers in the generator: %d\\n\\n\" % num_layers_gen)\n",
    "\n",
    "run_gan_code(discriminator=discriminator, generator=generator, results_dir=\"results_DG1\", dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0934e152-349a-4ce1-bcac-6c733f4f9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to generate 1000 images from the saved generator model\n",
    "generator.load_state_dict(torch.load(\"/content/drive/MyDrive/DLStudio-2.4.2/ExamplesAdversarialLearning/results_DG1/save_model_loss_1\"))\n",
    "input_noise = torch.randn(1000, 100, 1, 1, device=torch.device(\"cpu\"))\n",
    "fake_imgs = []\n",
    "j = 1\n",
    "with torch.no_grad():\n",
    "  newfake = generator(input_noise).detach().cpu()\n",
    "fake_imgs.append(newfake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58700f0-1a2a-4fc7-b53a-623f09a4827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to save the generated images\n",
    "for i in range(len(fake_imgs[0])):\n",
    "  image = fake_imgs[0][i].permute(1, 2, 0).cpu().detach().numpy()\n",
    "  plt.imshow(image)\n",
    "  plt.axis('off')  # Hide axis\n",
    "  plt.savefig(f'/content/drive/MyDrive/DLStudio-2.4.2/ExamplesAdversarialLearning/results_DG1/gen_fake_images1/image_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce2e122-760d-4e0a-8768-c75622020f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the list of image paths for FID error calculation\n",
    "def list_files_in_folder(folder_path):\n",
    "    files = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if os.path.isfile(os.path.join(folder_path, file_name)):\n",
    "            files.append(os.path.join(folder_path, file_name))\n",
    "    return files\n",
    "\n",
    "real_folder_path = '/content/drive/MyDrive/DLStudio-2.4.2/ExamplesAdversarialLearning/celeba_dataset_64x64/0'\n",
    "fake_folder_path_1 = '/content/drive/MyDrive/DLStudio-2.4.2/ExamplesAdversarialLearning/results_DG1/gen_fake_images1'\n",
    "\n",
    "real_image_paths = list_files_in_folder(real_folder_path)\n",
    "fake_image_paths_1 = list_files_in_folder(fake_folder_path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8c49e-f05d-4e88-819a-7169ed236002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FID for GAN\n",
    "from pytorch_fid.fid_score import calculate_activation_statistics, calculate_frechet_distance\n",
    "from pytorch_fid.inception import InceptionV3\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dims = 2048\n",
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "model = InceptionV3([block_idx]).to(device)\n",
    "m1 , s1 = calculate_activation_statistics(real_image_paths , model , device = device )\n",
    "m2 , s2 = calculate_activation_statistics(fake_image_paths_1 , model , device = device )\n",
    "fid_value = calculate_frechet_distance(m1 , s1 , m2 , s2)\n",
    "print(f'FID: { fid_value: .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5702c-d20f-4cca-9885-bcac04a492cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying sample of generated images\n",
    "def display_random_images(folder_path, num_images=16, rows=4, cols=4):\n",
    "    # List all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    # Select num_images random files\n",
    "    random_files = random.sample(files, num_images)\n",
    "    \n",
    "    # Create a subplot grid with the specified number of rows and columns\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Open and display each image\n",
    "        img_path = os.path.join(folder_path, random_files[i])\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')  # Turn off axis labels\n",
    "    plt.show()\n",
    "\n",
    "folder_path_gan = \"/content/drive/MyDrive/DLStudio-2.4.2/ExamplesAdversarialLearning/results_DG1/gen_fake_images1\"\n",
    "display_random_images(folder_path_gan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
